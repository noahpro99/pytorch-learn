{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/repos/pytorch-learn/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50001it [00:07, 6973.79it/s]\n"
     ]
    }
   ],
   "source": [
    "all_chars = set()\n",
    "data = []\n",
    "with open(\"../data/text8/text8\") as f:\n",
    "    # read chunks but not by line since text8 doesn't have lines\n",
    "    for i, chunk in tqdm(enumerate(iter(lambda: f.read(1024), ''))):\n",
    "        # words = nltk.word_tokenize(chunk)\n",
    "        chars = [c for c in chunk]\n",
    "        all_chars.update(chars)\n",
    "        for char in chars:\n",
    "            data.append(char)\n",
    "        if i > 50000:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to a file text8-small\n",
    "with open(\"../data/text8/text8-small\", \"w\") as f:\n",
    "    f.write(\"\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 'm',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 'g',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'm',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'g',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'c',\n",
       " 'l',\n",
       " 'a',\n",
       " 's',\n",
       " 's',\n",
       " ' ',\n",
       " 'r',\n",
       " 'a',\n",
       " 'd',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'l',\n",
       " 'u',\n",
       " 'd',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 'm',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 'g',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'm',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'g',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'c',\n",
       " 'l',\n",
       " 'a',\n",
       " 's',\n",
       " 's',\n",
       " ' ',\n",
       " 'r',\n",
       " 'a',\n",
       " 'd',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'l',\n",
       " 'u',\n",
       " 'd',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'd',\n",
       " 'i',\n",
       " 'g',\n",
       " 'g',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'n',\n",
       " 'g',\n",
       " 'l',\n",
       " 'i',\n",
       " 's',\n",
       " 'h',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'v',\n",
       " 'o',\n",
       " 'l',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 'c',\n",
       " 'u',\n",
       " 'l',\n",
       " 'o',\n",
       " 't',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'h',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'v',\n",
       " 'o',\n",
       " 'l',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'w',\n",
       " 'h',\n",
       " 'i',\n",
       " 'l',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'm',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'p',\n",
       " 'e',\n",
       " 'j',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 's',\n",
       " 'c',\n",
       " 'r',\n",
       " 'i',\n",
       " 'b',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'y',\n",
       " ' ',\n",
       " 'a',\n",
       " 'c',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'v',\n",
       " 'i',\n",
       " 'o',\n",
       " 'l',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'o',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " 'g',\n",
       " 'a',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'e',\n",
       " 't',\n",
       " 'y',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'l',\n",
       " 's',\n",
       " 'o',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'a',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'u',\n",
       " 'p',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'p',\n",
       " 'o',\n",
       " 's',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 'b',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 's',\n",
       " 'e',\n",
       " 'l',\n",
       " 'f',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 'm',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'f',\n",
       " 'r',\n",
       " 'o',\n",
       " 'm',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'g',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " 'k',\n",
       " ' ',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 'r',\n",
       " 'u',\n",
       " 'l',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 'e',\n",
       " 'f',\n",
       " ' ',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 'm',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'p',\n",
       " 'o',\n",
       " 'l',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'p',\n",
       " 'h',\n",
       " 'i',\n",
       " 'l',\n",
       " 'o',\n",
       " 's',\n",
       " 'o',\n",
       " 'p',\n",
       " 'h',\n",
       " 'y',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'l',\n",
       " 'i',\n",
       " 'e',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'r',\n",
       " 'u',\n",
       " 'l',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'n',\n",
       " 'e',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'h',\n",
       " 'o',\n",
       " 'u',\n",
       " 'l',\n",
       " 'd',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'o',\n",
       " 'l',\n",
       " 'i',\n",
       " 's',\n",
       " 'h',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'l',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'u',\n",
       " 'g',\n",
       " 'h',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'd',\n",
       " 'i',\n",
       " 'f',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 't',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'w',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 'm',\n",
       " ' ',\n",
       " 'a',\n",
       " 'l',\n",
       " 's',\n",
       " 'o',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 'd',\n",
       " 'v',\n",
       " 'o',\n",
       " 'c',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'l',\n",
       " 'i',\n",
       " 'm',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'a',\n",
       " 'u',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 't',\n",
       " 'a',\n",
       " 'r',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'i',\n",
       " 't',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'u',\n",
       " 'l',\n",
       " 'a',\n",
       " 'r',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'y',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'h',\n",
       " 'a',\n",
       " 'o',\n",
       " 's',\n",
       " ' ',\n",
       " 'n',\n",
       " 'i',\n",
       " 'h',\n",
       " 'i',\n",
       " 'l',\n",
       " 'i',\n",
       " 's',\n",
       " 'm',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " 'm',\n",
       " 'i',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 'r',\n",
       " 'a',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'r',\n",
       " 'm',\n",
       " 'o',\n",
       " 'n',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 't',\n",
       " 'i',\n",
       " ' ',\n",
       " 'a',\n",
       " 'u',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 't',\n",
       " 'a',\n",
       " 'r',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'e',\n",
       " 't',\n",
       " 'y',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'p',\n",
       " 'l',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'w',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'g',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'u',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'r',\n",
       " 'i',\n",
       " 't',\n",
       " 'a',\n",
       " 'r',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " ' ',\n",
       " 'p',\n",
       " 'o',\n",
       " 'l',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'u',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'e',\n",
       " 'r',\n",
       " 'c',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'o',\n",
       " 'm',\n",
       " 'i',\n",
       " 'c',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'i',\n",
       " 't',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " ...]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten works [' ', 'a', 'n', 'a', 'r', 'c', 'h', 'i', 's', 'm', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 't', 'e']\n",
      "first ten tokens [0, 13, 11, 13, 23, 17, 26, 16, 8, 9, 0, 7, 23, 16, 15, 16, 11, 13, 4, 22]\n",
      "first ten detokens [' ', 'a', 'n', 'a', 'r', 'c', 'h', 'i', 's', 'm', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 't', 'e']\n"
     ]
    }
   ],
   "source": [
    "str_to_token = {word: i for i, word in enumerate(all_chars)}\n",
    "token_to_str = {i: word for i, word in enumerate(all_chars)}\n",
    "tokenizer = lambda x: [str_to_token[word] for word in x]\n",
    "detokenizer = lambda x: [token_to_str[word] for word in x]\n",
    "\n",
    "first_few = data[:20]\n",
    "print(f\"first ten works {first_few}\")\n",
    "print(f\"first ten tokens {tokenizer(first_few)}\")\n",
    "print(f\"first ten detokens {detokenizer(tokenizer(first_few))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WINDOW = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take chunks of exactly CONTEXT_WINDOW words and chuck everything else and flatten to a single list of these\n",
    "examples = [tokenizer(data[i:i+CONTEXT_WINDOW]) for i in range(0, len(data), CONTEXT_WINDOW) if i+CONTEXT_WINDOW < len(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 23,\n",
       " 17,\n",
       " 26,\n",
       " 16,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 23,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 11,\n",
       " 13,\n",
       " 4,\n",
       " 22,\n",
       " 10,\n",
       " 0,\n",
       " 13,\n",
       " 8,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 4,\n",
       " 22,\n",
       " 23,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 19,\n",
       " 0,\n",
       " 13,\n",
       " 14,\n",
       " 21,\n",
       " 8,\n",
       " 22,\n",
       " 0,\n",
       " 19,\n",
       " 16,\n",
       " 23,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 21,\n",
       " 8,\n",
       " 22,\n",
       " 10,\n",
       " 0,\n",
       " 13,\n",
       " 15,\n",
       " 13,\n",
       " 16,\n",
       " 11,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 22,\n",
       " 13,\n",
       " 23,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 25,\n",
       " 7,\n",
       " 23,\n",
       " 1,\n",
       " 16,\n",
       " 11,\n",
       " 15,\n",
       " 0,\n",
       " 17,\n",
       " 5,\n",
       " 13,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 23,\n",
       " 13,\n",
       " 10,\n",
       " 16,\n",
       " 17,\n",
       " 13,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 16,\n",
       " 11,\n",
       " 17,\n",
       " 5,\n",
       " 21,\n",
       " 10,\n",
       " 16,\n",
       " 11,\n",
       " 15,\n",
       " 0,\n",
       " 4]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512020, 100])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(examples).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * CONTEXT_WINDOW\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example_idx = index // CONTEXT_WINDOW\n",
    "        inside_example_idx = index % CONTEXT_WINDOW\n",
    "        x = torch.zeros(CONTEXT_WINDOW, dtype=torch.long, device=device)\n",
    "        x[CONTEXT_WINDOW-inside_example_idx:] = self.data[example_idx][:inside_example_idx]\n",
    "        y = self.data[example_idx][inside_example_idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409616, 102404)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = int(0.8 * len(examples))\n",
    "# shuffle order first\n",
    "# examples = sklearn.utils.shuffle(examples)\n",
    "train_rows = examples[:split]\n",
    "test_rows = examples[split:]\n",
    "len(train_rows), len(test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40961600, 10240400)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = CustomDataset(train_rows)\n",
    "test_data = CustomDataset(test_rows)\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  6, 22, 11,  0, 14, 16, 23,  4, 26,  8,  0,  7, 11, 22,  0, 12, 22,\n",
       "         23,  7,  0, 12, 22, 23,  7,  0, 12, 22, 23,  7,  0, 24,  7, 24, 21,  5,\n",
       "         13,  4, 16,  7, 11,  0,  4, 25,  7,  0, 12, 22, 23,  7,  0, 12, 22, 23,\n",
       "          7,  0, 19, 16,  6, 22,  0, 22,  8,  4,  0, 10, 22, 13,  4, 26,  0, 23,\n",
       "         13,  4, 22,  0,  7, 11, 22,  0, 19, 16,  6, 22,  0, 19,  7, 21, 23,  0,\n",
       "         10, 22, 13,  4, 26,  8,  0,  7, 11, 22]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample a random train data point\n",
    "i = np.random.randint(len(train_data))\n",
    "x, y = train_data[i]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 100]), torch.Size([10]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[2, 0, 1]])\n",
      "after embedding tensor([[[ 2.2838,  0.3323, -0.0921],\n",
      "         [-0.0763,  0.6336,  0.8634],\n",
      "         [ 1.7532, -1.5495,  0.1196]]], grad_fn=<EmbeddingBackward0>)\n",
      "keys shape tensor([[[-0.0933, -0.1292,  0.5140,  0.4969],\n",
      "         [ 0.3470, -0.0704, -0.5814, -0.5986],\n",
      "         [-0.4484,  0.6269,  0.5217,  0.0563]]], grad_fn=<ViewBackward0>)\n",
      "queries shape tensor([[[-1.2417, -0.1688,  0.8942, -1.4008],\n",
      "         [-1.0212,  0.1979,  0.6813, -0.2369],\n",
      "         [-0.2959,  0.4723,  0.1512, -0.8121]]], grad_fn=<ViewBackward0>)\n",
      "values shape tensor([[[ 1.1759, -0.4907, -0.5228],\n",
      "         [ 0.1848, -0.0240,  0.0080],\n",
      "         [ 1.2319,  0.1147,  0.0014]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0988, -0.1003,  0.8386],\n",
       "         [ 0.3022, -0.6225,  0.9241],\n",
       "         [-0.3592,  0.2623,  0.4619]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(3, (1,3), dtype=torch.long).to(device)\n",
    "print(f\"x {x}\")\n",
    "embedding_dim = 3\n",
    "embedding = nn.Embedding(len(all_chars), embedding_dim).to(device)\n",
    "x = embedding(x).to(device)\n",
    "print(f\"after embedding {x}\")\n",
    "\n",
    "hidden_units = 4\n",
    "keys = nn.Linear(embedding_dim, hidden_units).to(device)\n",
    "queries = nn.Linear(embedding_dim, hidden_units).to(device)\n",
    "values = nn.Linear(embedding_dim, embedding_dim).to(device)\n",
    "K: torch.Tensor = keys(x)\n",
    "Q: torch.Tensor = queries(x)\n",
    "V: torch.Tensor = values(x)\n",
    "print(f\"keys shape {K}\")\n",
    "print(f\"queries shape {Q}\")\n",
    "print(f\"values shape {V}\")\n",
    "\n",
    "torch.matmul(Q, K.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int) -> None:\n",
    "        super().__init__()\n",
    "        self.keys = nn.Linear(input_shape, hidden_units)\n",
    "        self.queries = nn.Linear(input_shape, hidden_units)\n",
    "        self.value_down = nn.Linear(input_shape, hidden_units)\n",
    "        self.value_up = nn.Linear(hidden_units, input_shape)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(input_shape, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        K: torch.Tensor = self.keys(x)\n",
    "        Q: torch.Tensor = self.queries(x)\n",
    "        V: torch.Tensor = self.value_up(self.value_down(x))\n",
    "\n",
    "        top = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention = top / self.sqrt_d\n",
    "        attention = torch.nn.functional.softmax(attention, dim=-1)\n",
    "        return torch.matmul(attention, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, num_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(input_shape, hidden_units) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(hidden_units * num_heads, input_shape)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(torch.cat([head(x) for head in self.heads], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, num_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(input_shape, hidden_units, num_heads)\n",
    "        self.linear = nn.Linear(input_shape, input_shape)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        before = x.clone()\n",
    "        x = self.multi_head_attention(x)\n",
    "        x = torch.nn.functional.layer_norm(x + before, x.shape)\n",
    "        before = x.clone()\n",
    "        x = self.linear(x)\n",
    "        x = torch.nn.functional.layer_norm(x + before, x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_shape: int, embedding_dim: int, hidden_units: int, num_encoders: int, num_heads: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_shape, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.encoders = nn.ModuleList(\n",
    "            [EncoderLayer(embedding_dim, hidden_units, num_heads) for _ in range(num_encoders)]\n",
    "        )\n",
    "        \n",
    "        self.final_layer = nn.Linear(in_features=hidden_units*CONTEXT_WINDOW, out_features=input_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(all_chars)\n",
    "model_0 = CustomModel(\n",
    "    input_shape=vocab_size,\n",
    "    embedding_dim=100,\n",
    "    hidden_units=100,\n",
    "    num_encoders=10,\n",
    "    num_heads=10\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5_414_727'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = 0\n",
    "for x in model_0.parameters():\n",
    "    total_params += np.array(x.shape).prod()\n",
    "    \n",
    "f\"{total_params:_}\" # 40 961 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"../runs/transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100]) torch.Size([10, 27]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# make example prediction\n",
    "\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_dataloader:\n",
    "        y_preds = model_0(X)\n",
    "        print(X.shape, y_preds.shape, y.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    accuracy_fn,\n",
    "):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for X, y in tqdm(dataloader):\n",
    "        model.train()\n",
    "        y_logits = model(X)\n",
    "        y_preds = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)\n",
    "        loss = loss_fn(y_logits, y)\n",
    "        acc = accuracy_fn(y_true=y.cpu(), y_pred=y_preds.cpu())\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    accuracy_fn,\n",
    "):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in tqdm(dataloader):\n",
    "            y_logits = model(X)\n",
    "            y_preds = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)\n",
    "            loss = loss_fn(y_logits, y)\n",
    "            acc = accuracy_fn(y_true=y.cpu(), y_pred=y_preds.cpu())\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_acc += acc\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    accuracy_score: callable,\n",
    "    epochs: int = 3,\n",
    ") -> dict[str, list[float]]:\n",
    "\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "    }\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        train_loss, train_acc = train_step(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            accuracy_fn=accuracy_score,\n",
    "        )\n",
    "\n",
    "        test_loss, test_acc = test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=accuracy_score,\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model_0_train_results \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_0,\n\u001b[1;32m      6\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:284\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    281\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/__init__.py:64\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_builtins\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Wrap manual_seed with the disable decorator.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Can't do it at its implementation due to dependency issues.\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed \u001b[38;5;241m=\u001b[39m \u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Add the new manual_seed to the builtin registry.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_builtins\u001b[38;5;241m.\u001b[39m_register_builtin(torch\u001b[38;5;241m.\u001b[39mmanual_seed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maten::manual_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/decorators.py:50\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     48\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:410\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 410\u001b[0m     (filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtrace_rules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_wrapped_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    413\u001b[0m     )\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m DONT_WRAP_FILES\n\u001b[1;32m    415\u001b[0m ):\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# call to a builtin without a frame for us to capture\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     fn \u001b[38;5;241m=\u001b[39m external_utils\u001b[38;5;241m.\u001b[39mwrap_inline(fn)\n\u001b[1;32m    419\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:3378\u001b[0m, in \u001b[0;36mcheck\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(obj, is_inlined_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inlined_call\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mskipped\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:3361\u001b[0m, in \u001b[0;36mcheck_verbose\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m   3358\u001b[0m     fi \u001b[38;5;241m=\u001b[39m FunctionInfo(obj, \u001b[38;5;28;01mNone\u001b[39;00m, getfile(obj), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;66;03m# Consulte the central trace rules defined in torch._dynamo.trace_rules.\u001b[39;00m\n\u001b[0;32m-> 3361\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_rules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_inner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inlined_call\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m [UserFunctionVariable, FunctorchHigherOrderVariable]:\n\u001b[1;32m   3365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkipResult(\n\u001b[1;32m   3366\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minlined according trace_rules.lookup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3368\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:3442\u001b[0m, in \u001b[0;36mlookup_inner\u001b[0;34m(obj, name, filename, is_direct_call)\u001b[0m\n\u001b[1;32m   3440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_aten_op_or_tensor_method(obj):\n\u001b[1;32m   3441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TorchInGraphFunctionVariable\n\u001b[0;32m-> 3442\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[43mget_torch_obj_rule_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(obj, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rule\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2782\u001b[0m, in \u001b[0;36mget_torch_obj_rule_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m torch_name_rule_map:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 2782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mload_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2784\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m d \u001b[38;5;129;01mand\u001b[39;00m d[obj] \u001b[38;5;241m!=\u001b[39m v:\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2811\u001b[0m, in \u001b[0;36mload_object\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2809\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2810\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid obj name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2811\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43m_load_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2812\u001b[0m     val \u001b[38;5;241m=\u001b[39m unwrap_if_wrapper(val)\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "File \u001b[0;32m~/repos/pytorch-learn/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2795\u001b[0m, in \u001b[0;36m_load_obj_from_str\u001b[0;34m(fully_qualified_name)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_obj_from_str\u001b[39m(fully_qualified_name):\n\u001b[1;32m   2794\u001b[0m     module, obj_name \u001b[38;5;241m=\u001b[39m fully_qualified_name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m, obj_name)\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:117\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Import a module.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mThe 'package' argument is required when performing a relative import. It\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m package:\n\u001b[1;32m    119\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpackage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument is required to perform a relative \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport for \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.01)\n",
    "\n",
    "model_0_train_results = train_model(\n",
    "    model=model_0,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    accuracy_score=accuracy_score,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"../runs/model_0\")\n",
    "\n",
    "for epoch in range(len(model_0_train_results[\"train_loss\"])):\n",
    "    writer.add_scalars(\n",
    "        \"loss\",\n",
    "        {\n",
    "            \"train_loss\": model_0_train_results[\"train_loss\"][epoch],\n",
    "            \"test_loss\": model_0_train_results[\"test_loss\"][epoch],\n",
    "        },\n",
    "        epoch,\n",
    "    )\n",
    "    writer.add_scalars(\n",
    "        \"accuracy\",\n",
    "        {\n",
    "            \"train_acc\": model_0_train_results[\"train_acc\"][epoch],\n",
    "            \"test_acc\": model_0_train_results[\"test_acc\"][epoch],\n",
    "        },\n",
    "        epoch,\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting loss and accuracy\n",
    "for i, metric in enumerate([\"loss\", \"acc\"]):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    for key in model_0_train_results:\n",
    "        if metric in key:\n",
    "            plt.plot(\n",
    "                range(len(model_0_train_results[key])),\n",
    "                model_0_train_results[key],\n",
    "                label=key,\n",
    "            )\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.title(f\"Train {metric.capitalize()}\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
